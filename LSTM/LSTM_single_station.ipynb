{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "# Get the current time\n",
        "current_time = datetime.now()\n",
        "# Print the current time\n",
        "print(\"Current time:\", current_time)\n",
        "!pip install dataretrieval\n",
        "from dataretrieval import nwis\n",
        "import random\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install hydroeval\n",
        "import hydroeval as he\n",
        "# Set seeds for all major libraries\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "d5MsLN8ehSKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f6ab1c-5fb6-4602-df8a-a2be96d85cf3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current time: 2025-05-24 20:28:40.219592\n",
            "Requirement already satisfied: dataretrieval in /usr/local/lib/python3.11/dist-packages (1.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dataretrieval) (2.32.3)\n",
            "Requirement already satisfied: pandas==2.* in /usr/local/lib/python3.11/dist-packages (from dataretrieval) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.*->dataretrieval) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.*->dataretrieval) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.*->dataretrieval) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.*->dataretrieval) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dataretrieval) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dataretrieval) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dataretrieval) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dataretrieval) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.*->dataretrieval) (1.17.0)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: hydroeval in /usr/local/lib/python3.11/dist-packages (0.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from hydroeval) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Generate Sample Data (Replace with your actual DataFrame) ---\n",
        "# Assuming your DataFrame 'df' is already loaded.\n",
        "# For demonstration, let's create a sample DataFrame:\n",
        "#np.random.seed(42) # for reproducibility\n",
        "#dates = pd.to_datetime(pd.date_range(start='2020-01-01', periods=200, freq='D'))\n",
        "#x1 = np.sin(np.linspace(0, 50, 200)) + np.random.rand(200) * 0.5\n",
        "#x2 = np.cos(np.linspace(0, 40, 200)) * 0.8 + np.random.rand(200) * 0.3\n",
        "#y = 0.7 * x1 + 0.3 * x2 + np.sin(np.linspace(0, 60, 200)) * 0.5 + np.random.rand(200) * 0.2\n",
        "#df = pd.DataFrame({'date': dates, 'x1': x1, 'x2': x2, 'y': y})\n",
        "\n",
        "#print(\"Sample DataFrame Head:\")\n",
        "#print(df.head())\n",
        "#print(\"\\nDataFrame Info:\")\n",
        "#df.info()"
      ],
      "metadata": {
        "id": "pc5yDi0Wh4YE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "startDate = \"1995-10-01\"\n",
        "endDate = \"2024-09-30\"\n",
        "model_site = [\n",
        "         '13342500' # '13154500' #'12322000' #'12305000' # '13310850' #'13340000' #'13176400'  #'13340000'#'13176400' # #\"13311450\" # \"13296000\"\n",
        "]\n",
        "units_day = 365\n",
        "missing_data_threshold = 0.90 *365\n",
        "\n",
        "summer_months = [6,7,8]\n",
        "summer_missing_data_threshold = 0.90 * (30+31+31)\n",
        "\n",
        "site_info = nwis.get_info(sites= model_site )\n",
        "site_info_df = pd.DataFrame( site_info[0] )\n",
        "print(\"site_info_df:\",site_info_df)\n",
        "print(\"station_nm:\",site_info_df['station_nm'][0])\n",
        "site_info_df[\"site_no\"]=site_info_df[\"site_no\"].astype(str)\n",
        "columns_to_save = ['site_no', 'station_nm','dec_lat_va', 'dec_long_va', 'drain_area_va', 'contrib_drain_area_va' ,   'alt_va'\n",
        "                  # ,'alt_meth_cd', 'alt_acy_va', 'alt_datum_cd'\n",
        "                  ]\n",
        "filtered_df = site_info_df[columns_to_save]\n",
        "\n",
        "\n",
        "\n",
        "parameterCode = [\"00010\",\"00060\"  ]\n",
        "dailyStreamtemp = nwis.get_dv(sites= model_site\n",
        "                              , parameterCd=parameterCode, start=startDate, end=endDate)\n",
        "dailyStreamtemp_df = pd.DataFrame( dailyStreamtemp[0] )\n",
        "dailyStreamtemp_df = dailyStreamtemp_df.reset_index()\n",
        "dailyStreamtemp_df['Date'] = dailyStreamtemp_df['datetime'].dt.date\n",
        "dailyStreamtemp_df['Month'] = dailyStreamtemp_df['datetime'].dt.month\n",
        "dailyStreamtemp_df['Year'] = dailyStreamtemp_df['datetime'].dt.year\n",
        "\n",
        "shared_url = \"https://drive.google.com/file/d/1z9w4rk21CCbbuOjwsru1pgkiOsYgVX6h/view?usp=share_link\"\n",
        "file_id = shared_url.split('/d/')[1].split('/')[0]\n",
        "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "prism_df = pd.read_csv(download_url)\n",
        "prism_df['date'] = pd.to_datetime( prism_df['date'])\n",
        "\n",
        "def days_since_october_first(date):\n",
        "    october_first = pd.Timestamp(year=date.year, month=10, day=1)\n",
        "    days_difference = (date - october_first).days\n",
        "    return days_difference if days_difference >= 0 else days_difference + 365\n",
        "\n",
        "prism_df['Days_Since_October_1st'] =  prism_df['date'].apply(days_since_october_first)\n",
        "#prism_df[ \"rad_d\"   ] = 2 * np.pi * prism_df[ \"Days_Since_October_1st\"   ] /365\n",
        "prism_df[\"site_no\"] =prism_df[\"site_no\"].astype(str)\n",
        "# Add a leading 0 to strings with exactly 7 characters\n",
        "prism_df['site_no'] = prism_df['site_no'].apply(lambda x: '0' + x if len(x) == 7 else x)\n",
        "prism_df[\"site_no\"] =prism_df[\"site_no\"].astype(str)\n",
        "#print( prism_df)\n",
        "prism_df['Date'] = prism_df['date'].dt.date\n",
        "\n",
        "prism_df = prism_df[['tmean' , \"site_no\", 'Date'\n",
        "#, \"rad_d\"\n",
        " ]]\n",
        "\n",
        "dailyStreamtemp_df = dailyStreamtemp_df[[    'Date',\n",
        "       'site_no',\n",
        "  '00010_Mean', '00060_Mean'\n",
        "  #,      'Month', 'Year'\n",
        "   ]]\n",
        "\n",
        "result = pd.merge(prism_df, dailyStreamtemp_df, on=['Date','site_no'], how='inner')\n",
        "\n",
        "# Count the number of NaN values in each column\n",
        "na_count = result.isna().sum()\n",
        "\n",
        "# Drop rows with any NaN values\n",
        "df_cleaned = result.dropna()\n",
        "\n",
        "df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'])\n",
        "\n",
        "train_month =  list(range( 3 , 8+1 ))\n",
        "\n",
        "\n",
        "print(\"train_month:\", train_month)\n",
        "\n",
        "df_cleaned= df_cleaned[ df_cleaned['Date'].dt.month.isin( train_month )]\n",
        "\n",
        "# Print the cleaned DataFrame\n",
        "print(\"df_cleaned:\",df_cleaned)\n",
        "\n",
        "# Count the number of NaN values in each column\n",
        "na_count = df_cleaned.isna().sum()\n",
        "\n",
        "print(na_count)"
      ],
      "metadata": {
        "id": "5iMKiUOajbuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#features = ['x1', 'x2']\n",
        "#target = 'y'\n",
        "# Normalize the data\n",
        "# It's crucial to normalize all input features and the target variable for LSTMs.\n",
        "# We'll use a separate scaler for the target 'y' to easily inverse transform predictions.\n",
        "#scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
        "#scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Fit and transform features\n",
        "#df[features] = scaler_features.fit_transform(df[features])\n",
        "\n",
        "# Fit and transform target\n",
        "# Reshape for scaler (expects 2D array)\n",
        "#df[target] = scaler_target.fit_transform(df[target].values.reshape(-1, 1))\n",
        "\n",
        "#print(\"\\nDataFrame after Normalization Head:\")\n",
        "#print(df.head())"
      ],
      "metadata": {
        "id": "xPuu58AznGi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Preprocessing ---\n",
        "# Select features and target\n",
        "features = ['tmean',  '00060_Mean' ]\n",
        "target = '00010_Mean'\n",
        "# Normalize the data\n",
        "# It's crucial to normalize all input features and the target variable for LSTMs.\n",
        "# We'll use a separate scaler for the target 'y' to easily inverse transform predictions.\n",
        "scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "print(df_cleaned.shape)\n",
        "df = df_cleaned\n",
        "print(df.shape)\n",
        "# Fit and transform features\n",
        "df[features] = scaler_features.fit_transform(df[features])\n",
        "\n",
        "# Fit and transform target\n",
        "# Reshape for scaler (expects 2D array)\n",
        "df[target] = scaler_target.fit_transform(df[target].values.reshape(-1, 1))\n",
        "\n",
        "print(\"\\nDataFrame after Normalization Head:\")\n",
        "print(df.head())\n",
        "print(df.tail())\n",
        "print(df.shape)"
      ],
      "metadata": {
        "id": "FOXpjpkuiDYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. Create Sequences for LSTM ---\n",
        "# LSTMs require data in a 3D format: (samples, timesteps, features)\n",
        "# 'timesteps' (also known as look_back) determines how many previous time steps\n",
        "# the LSTM will consider to predict the next value.\n",
        "\n",
        "look_back = 90 # You can adjust this based on your data's seasonality/dependencies\n",
        "\n",
        "def create_sequences(data, look_back, features_cols, target_col):\n",
        "    X, Y = [], []\n",
        "    # Ensure data is a numpy array for efficient indexing\n",
        "    data_np = data[features_cols + [target_col]].values # include target for sequence alignment\n",
        "    #print(data.head)\n",
        "    print(data_np)\n",
        "    # Features are the first 'num_features' columns, target is the last column.\n",
        "    num_features = len(features_cols)\n",
        "\n",
        "    for i in range(len(data_np) - look_back):\n",
        "        # X: look_back historical features (x1, x2)\n",
        "        X.append(data_np[i:(i + look_back), :num_features])\n",
        "        # Y: the 'y' value at the current time step (i + look_back)\n",
        "        Y.append(data_np[i + look_back, num_features]) # Target is the last column\n",
        "\n",
        "\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "X, Y = create_sequences(df, look_back, features, target)\n",
        "\n",
        "# Print first 4 elements\n",
        "print(\"First 4 Y elements:\", Y[:4])\n",
        "\n",
        "# Print last 4 elements\n",
        "print(\"Last 4 Y elements:\", Y[-4:])\n",
        "\n",
        "print(f\"\\nShape of X (samples, timesteps, features): {X.shape}\")\n",
        "print(f\"Shape of Y (samples, target_value): {Y.shape}\")"
      ],
      "metadata": {
        "id": "elVEmupBic0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. Train-Test Split (Time Series Split) ---\n",
        "# It's crucial to split time series data chronologically to avoid data leakage.\n",
        "train_size = int(len(X) * 0.8) # e.g., 80% for training, 20% for testing\n",
        "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
        "Y_train, Y_test = Y[0:train_size], Y[train_size:len(Y)]\n",
        "\n",
        "print(f\"Train samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")"
      ],
      "metadata": {
        "id": "YshtrOaoinrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Build LSTM Model ---\n",
        "h1= 25\n",
        "h2 =  12\n",
        "learning_rate = 0.001\n",
        "optimizer=    Adam(learning_rate= learning_rate ) #'adam'\n",
        "d1 = 0.05\n",
        "d2 = 0.005\n",
        "\n",
        "model = Sequential()\n",
        "# First LSTM layer with return_sequences=True to pass sequences to the next LSTM layer\n",
        "model.add(LSTM(units= h1, return_sequences=True, input_shape=(look_back, len(features))))\n",
        "model.add(Dropout( d1 )) # Dropout to prevent overfitting\n",
        "\n",
        "# Second LSTM layer\n",
        "model.add(LSTM(units= h2, return_sequences=False)) # return_sequences=False for the last LSTM layer\n",
        "model.add(Dropout( d2 ))\n",
        "\n",
        "# Dense output layer for regression (predicting a single value 'y')\n",
        "model.add(Dense(units=1))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "for i, layer in enumerate(model.layers):\n",
        "    print(f\"Layer {i}: {layer.__class__.__name__}, Activation: {getattr(layer, 'activation', 'N/A')}\")\n"
      ],
      "metadata": {
        "id": "f50G0eMkitVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Train the Model ---\n",
        "# EarlyStopping to stop training when a monitored metric has stopped improving.\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "batch =  8\n",
        "history = model.fit(X_train, Y_train,\n",
        "                    epochs=100, # You can increase epochs, EarlyStopping will handle it\n",
        "                    batch_size= batch ,\n",
        "                    validation_split=0.2, # Use a portion of training data for validation\n",
        "                    callbacks=[early_stopping],\n",
        "                    verbose=1)\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3lt-6gPzi3Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "gYi5pMG5s5bs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Evaluate the Model ---\n",
        "train_loss = model.evaluate(X_train, Y_train, verbose=0)\n",
        "test_loss = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(f\"\\nTrain Loss (MSE): {train_loss:.4f}\")\n",
        "print(f\"Test Loss (MSE): {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "ULMMcHfOjAL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Make Predictions ---\n",
        "# Make predictions on the test set\n",
        "Y_pred_scaled = model.predict(X_test)\n",
        "print(f\"\\nShape of Y_pred_scaled: {Y_pred_scaled.shape}\")\n",
        "print(f\"Shape of Y_test: {Y_test.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "\n",
        "# Inverse transform the predictions and actual values to the original scale\n",
        "Y_pred = scaler_target.inverse_transform(Y_pred_scaled)\n",
        "Y_test_original = scaler_target.inverse_transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "r2 = r2_score( Y_test_original ,  Y_pred )\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(Y_test_original, Y_pred))\n",
        "print(f\"Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "bias =np.mean( Y_pred- Y_test_original)\n",
        "print(f\"Test bias: { bias:.4f}\")\n",
        "\n",
        "# KGE calculation using hydroeval\n",
        "# Note: hydroeval's kge function returns KGE, r, alpha, beta by default\n",
        "kge_value, r_comp, alpha_comp, beta_comp = he.kge( Y_pred , Y_test_original)\n",
        "print( kge_value)\n"
      ],
      "metadata": {
        "id": "0KI7OCEwrFYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Y_pred\n",
        "#Y_test_original\n",
        "#print(Y_pred .shape)\n",
        "#df_cleaned[train_size:len(Y)]['Date']\n",
        "\n",
        "\n",
        "#dt_ys = df_cleaned[train_size:len(Y)]#['Date']\n",
        "\n",
        "print(df_cleaned.tail)\n",
        "print((len(Y)-Y_test.shape[0]  ))\n",
        "print(len(Y) )\n",
        "dt_ys = df_cleaned.iloc[  ( df_cleaned.shape[0]-Y_test.shape[0]  )  : df_cleaned.shape[0] ]#['Date']\n",
        "\n",
        "\n",
        "print(dt_ys )\n",
        "dt_ys['Y_pred'] = Y_pred\n",
        "dt_ys['Y_test_original']= Y_test_original\n",
        "\n",
        "dt_ys[['Date','Y_pred', \t'Y_test_original']]\n",
        "\n",
        "dt_ys['Date'] = pd.to_datetime( dt_ys['Date'])\n",
        "\n",
        "summer_df = dt_ys[ dt_ys['Date'].dt.month.isin([6, 7, 8])]\n",
        "\n",
        "print(summer_df.tail())\n",
        "\n",
        "summer_df.to_csv( ( model_site[0] + \"summer_df.csv\") )\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(summer_df['Y_test_original'], summer_df['Y_pred']))\n",
        "print(f\"Test RMSE: {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "FBymFfrDrIvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot( summer_df['Y_test_original'], label='Actual Y')\n",
        "plt.plot( summer_df['Y_pred'], label='Predicted Y')\n",
        "plt.title('Y Prediction vs Actual')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Y Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DdBzSmlsvM5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 7))\n",
        "plt.scatter(summer_df['Date'], summer_df['Y_test_original'], label='Actual Y')\n",
        "plt.scatter(summer_df['Date'], summer_df['Y_pred'], label='Predicted Y')\n",
        "\n",
        "\n",
        "\n",
        "plt.title('Y Prediction vs Actual')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Y Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "axNYDFZqxcoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small= summer_df[0:256]\n",
        "alpha=0.75\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.scatter( small['Date'],  small['Y_test_original'], label='Actual Y',alpha=alpha)\n",
        "plt.scatter( small['Date'],  small['Y_pred'], label='Predicted Y',alpha=alpha)\n",
        "\n",
        "\n",
        "\n",
        "plt.title('Y Prediction vs Actual')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Y Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qoml0jdByXm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "small= summer_df[187:256]\n",
        "\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot( small['Date'],  small['Y_test_original'], label='Actual Y')\n",
        "plt.plot( small['Date'],  small['Y_pred'], label='Predicted Y')\n",
        "\n",
        "\n",
        "\n",
        "plt.title('Y Prediction vs Actual')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Y Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e6sAE2pEzR0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zgfvhtp5hNbC"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# --- 9. Visualize Predictions vs. Actual ---\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(Y_test_original, label='Actual Y' ,alpha=alpha )\n",
        "plt.plot(Y_pred, label='Predicted Y' ,alpha=alpha)\n",
        "plt.title('Y Prediction vs Actual')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Y Value')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Plot a smaller segment for better detail\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.plot(Y_test_original[1234:1456], label='Actual Y')\n",
        "plt.plot(Y_pred[ 1234:1456 ], label='Predicted Y')\n",
        "plt.title('Y Prediction vs Actual (Zoomed In)')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Y Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df = pd.DataFrame({\n",
        "                          \"model_site\": [model_site[0]],\n",
        "                          \"train_month\" : [train_month]\n",
        "   , \"h1\":[h1]\n",
        "                          ,\"h2\":[h2]\n",
        "                          ,\"d1\":[d1]\n",
        "                          ,\"d2\":[d2]\n",
        "                          ,\"batch\":[batch]\n",
        "                          ,\"look_back\":[look_back]\n",
        "\n",
        "\n",
        "                          ,\"optimizer\":[optimizer]\n",
        "    ,'learning_rate':[learning_rate]\n",
        "    ,\"r2\" :[r2]\n",
        "    , \"rmse\":[rmse]\n",
        "\n",
        "\n",
        "                           , \"bias\":[bias]\n",
        "    ,\"KGE\" : [kge_value]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    })\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "\n",
        "result_df.to_csv( model_site[0]+'_result_df.csv' , index=False)\n",
        "\n",
        "result_df.to_csv('/content/drive/My Drive/LSTM_results/' + model_site[0]+'_result_df_'+   current_time.strftime(\"%Y%m%d%H%M\") + '.csv' , index=False)\n"
      ],
      "metadata": {
        "id": "WL_dXfvak8_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save( (model_site[0]+'_lstm_model.h5') )"
      ],
      "metadata": {
        "id": "0aynKXXorpyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summer_df\n",
        "summer_df['bias'] = summer_df['Y_pred'] - summer_df['Y_test_original']\n",
        "print( summer_df.head() )\n",
        "\n",
        "# 2. Scatter plot: x = Y_test_original, y = Y_pred\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter( summer_df['Y_test_original'],  summer_df['Y_pred'], alpha=0.6, label='Predictions')\n",
        "plt.plot( summer_df['Y_test_original'],  summer_df['Y_test_original'], color='red', linestyle='--', label='Ideal (Y=X)')\n",
        "plt.xlabel('Y_test_original')\n",
        "plt.ylabel('Y_pred')\n",
        "plt.title('Scatter Plot of Predictions vs True Values')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 2. Scatter plot: x = Y_test_original, y = Y_pred\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter( summer_df['Y_test_original'],  summer_df['bias'], alpha=0.6, label='Predictions')\n",
        "\n",
        "# Calculate regression line\n",
        "coefficients = np.polyfit(  summer_df['Y_test_original'],  summer_df['bias'] , 1)  # 1 for linear regression\n",
        "polynomial = np.poly1d(coefficients)\n",
        "y_pred = polynomial(  summer_df['Y_test_original']  )\n",
        "# Plot regression line\n",
        "plt.plot(  summer_df['Y_test_original'] , y_pred, color='red', label=f'Regression line: y = {coefficients[0]:.2f}x + {coefficients[1]:.2f}')\n",
        "\n",
        "\n",
        "plt.xlabel('Y_test_original')\n",
        "plt.ylabel('Y_pred')\n",
        "plt.title('Scatter Plot of Predictions vs True Values')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dAiJsmazqIMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.sciencedirect.com/science/article/pii/S2214581824000648#sec0030"
      ],
      "metadata": {
        "id": "MaUoxsWn58_8"
      }
    }
  ]
}