{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "841b2ac1-b2ab-46a7-8098-7e511d1bbe05",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "841b2ac1-b2ab-46a7-8098-7e511d1bbe05",
        "outputId": "855bdd9e-ee5a-4e3c-b6ee-3ac7e7da5385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn==1.5.2 in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.5.2) (3.6.0)\n",
            "1.5.2\n",
            "Current time: 2025-06-26 01:28:38.539106\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import dataretrieval\n",
        "except ImportError:\n",
        "    print(\"dataretrieval not found. Installing...\")\n",
        "    !pip install dataretrieval\n",
        "    import dataretrieval # Import again after installation\n",
        "\n",
        "from dataretrieval import nwis # hydrological time-series data from USGS\n",
        "\n",
        "# Use scikit-learn to grid search the number of neurons\n",
        "!pip install scikit-learn==1.5.2 # TO AVOID AttributeError: 'super' object has no attribute '__sklearn_tags__'\n",
        "import sklearn\n",
        "print(sklearn.__version__)\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV  #  hyperparameter tuning\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adadelta\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "#!pip install scikeras\n",
        "#from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "try:\n",
        "    from scikeras.wrappers import KerasRegressor\n",
        "except ImportError:\n",
        "    print(\"scikeras not found. Installing...\")\n",
        "    !pip install scikeras\n",
        "    from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "from tensorflow.keras.constraints import MaxNorm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score # Imports several common regression evaluation metrics from scikit-learn:\n",
        "from datetime import datetime\n",
        "# Get the current time\n",
        "current_time = datetime.now()\n",
        "# Print the current time\n",
        "print(\"Current time:\", current_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0c41c89c-ca94-4e5d-ab16-6dea19a694cf",
      "metadata": {
        "id": "0c41c89c-ca94-4e5d-ab16-6dea19a694cf"
      },
      "outputs": [],
      "source": [
        "startDate = \"1995-10-01\"\n",
        "endDate = \"2024-09-30\"\n",
        "model_site = [\n",
        "  \"13190500\"\n",
        "]\n",
        "\n",
        "#units_day = 365\n",
        "#missing_data_threshold = 0.90 *365\n",
        "\n",
        "summer_months = [6,7,8]\n",
        "#summer_missing_data_threshold = 0.90 * (30+31+31)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f6872aff-2036-4015-8df3-03ca7b4ee76a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6872aff-2036-4015-8df3-03ca7b4ee76a",
        "outputId": "25cffdf3-86e9-4648-a21a-6e360a10236a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "site_info_df:   agency_cd   site_no                               station_nm site_tp_cd  \\\n",
            "0      USGS  13190500  SF BOISE RIVER AT ANDERSON RANCH DAM ID         ST   \n",
            "\n",
            "   lat_va  long_va  dec_lat_va  dec_long_va coord_meth_cd coord_acy_cd  ...  \\\n",
            "0  432037  1152839   43.343611    -115.4775             G            S  ...   \n",
            "\n",
            "  reliability_cd gw_file_cd  nat_aqfr_cd  aqfr_cd  aqfr_type_cd well_depth_va  \\\n",
            "0            NaN   NNNNNNNN          NaN      NaN           NaN           NaN   \n",
            "\n",
            "  hole_depth_va depth_src_cd  project_no                    geometry  \n",
            "0           NaN          NaN         NaN  POINT (-115.4775 43.34361)  \n",
            "\n",
            "[1 rows x 43 columns]\n",
            "station_nm: SF BOISE RIVER AT ANDERSON RANCH DAM ID\n",
            "train_month: [3, 4, 5, 6, 7, 8]\n",
            "df_cleaned:            tmean   site_no       Date  Days_Since_October_1st  00010_Mean  \\\n",
            "8918   -1.199000  13190500 2020-03-01                     151         4.0   \n",
            "8919   -1.367000  13190500 2020-03-02                     152         4.1   \n",
            "8920    3.940400  13190500 2020-03-03                     153         4.4   \n",
            "8921    2.841400  13190500 2020-03-04                     154         4.2   \n",
            "8922    3.875500  13190500 2020-03-05                     155         4.1   \n",
            "...          ...       ...        ...                     ...         ...   \n",
            "10558  17.757401  13190500 2024-08-27                     330         9.8   \n",
            "10559  13.613300  13190500 2024-08-28                     331         9.6   \n",
            "10560  14.822449  13190500 2024-08-29                     332         9.5   \n",
            "10561  18.287849  13190500 2024-08-30                     333         9.7   \n",
            "10562  22.055199  13190500 2024-08-31                     334         9.7   \n",
            "\n",
            "       00060_Mean  \n",
            "8918        305.0  \n",
            "8919        299.0  \n",
            "8920        302.0  \n",
            "8921        301.0  \n",
            "8922        295.0  \n",
            "...           ...  \n",
            "10558       563.0  \n",
            "10559       528.0  \n",
            "10560       529.0  \n",
            "10561       526.0  \n",
            "10562       526.0  \n",
            "\n",
            "[920 rows x 6 columns]\n",
            " df_cleaned.describe():             tmean                 Date  Days_Since_October_1st  00010_Mean  \\\n",
            "count  920.000000                  920              920.000000  920.000000   \n",
            "mean    12.770957  2022-05-31 16:48:00              242.500000    6.542174   \n",
            "min     -8.266300  2020-03-01 00:00:00              151.000000    3.400000   \n",
            "25%      5.592250  2021-04-15 18:00:00              196.750000    4.800000   \n",
            "50%     13.607650  2022-05-31 12:00:00              242.500000    6.100000   \n",
            "75%     20.683000  2023-07-16 06:00:00              288.250000    8.200000   \n",
            "max     27.730900  2024-08-31 00:00:00              334.000000   12.000000   \n",
            "std      8.652572                  NaN               53.144331    2.093101   \n",
            "\n",
            "        00060_Mean  \n",
            "count   920.000000  \n",
            "mean   1115.826087  \n",
            "min     267.000000  \n",
            "25%     322.000000  \n",
            "50%    1160.000000  \n",
            "75%    1380.000000  \n",
            "max    6390.000000  \n",
            "std     925.256824  \n",
            "tmean                     0\n",
            "site_no                   0\n",
            "Date                      0\n",
            "Days_Since_October_1st    0\n",
            "00010_Mean                0\n",
            "00060_Mean                0\n",
            "dtype: int64\n",
            "gaps          tmean   site_no       Date  Days_Since_October_1st  00010_Mean  \\\n",
            "9283  -3.77430  13190500 2021-03-01                     151         3.5   \n",
            "9648   6.26810  13190500 2022-03-01                     151         3.9   \n",
            "10013 -4.08060  13190500 2023-03-01                     151         4.0   \n",
            "10379  1.10605  13190500 2024-03-01                     151         4.8   \n",
            "\n",
            "       00060_Mean      Gap  \n",
            "9283        307.0 182 days  \n",
            "9648        309.0 182 days  \n",
            "10013       280.0 182 days  \n",
            "10379       300.0 183 days  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3-3487914925.py:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'])\n"
          ]
        }
      ],
      "source": [
        "site_info = nwis.get_info(sites= model_site )\n",
        "site_info_df = pd.DataFrame( site_info[0] )\n",
        "print(\"site_info_df:\",site_info_df)\n",
        "print(\"station_nm:\",site_info_df['station_nm'][0])\n",
        "site_info_df[\"site_no\"]=site_info_df[\"site_no\"].astype(str)\n",
        "columns_to_save = ['site_no', 'station_nm','dec_lat_va', 'dec_long_va', 'drain_area_va', 'contrib_drain_area_va' ,   'alt_va']\n",
        "filtered_df = site_info_df[columns_to_save]\n",
        "\n",
        "parameterCode = [\"00010\",\"00060\"  ]\n",
        "dailyStreamtemp = nwis.get_dv(sites= model_site\n",
        "                              , parameterCd=parameterCode, start=startDate, end=endDate)\n",
        "dailyStreamtemp_df = pd.DataFrame( dailyStreamtemp[0] )\n",
        "\n",
        "dailyStreamtemp_df = dailyStreamtemp_df.reset_index()\n",
        "# Extracts Date, Month, and Year components from the datetime column.\n",
        "dailyStreamtemp_df['Date'] = dailyStreamtemp_df['datetime'].dt.date\n",
        "dailyStreamtemp_df['Month'] = dailyStreamtemp_df['datetime'].dt.month\n",
        "dailyStreamtemp_df['Year'] = dailyStreamtemp_df['datetime'].dt.year\n",
        "\n",
        "dailyStreamtemp_df = dailyStreamtemp_df[ ~( (dailyStreamtemp_df[\"site_no\"]==\"13310800\") & (dailyStreamtemp_df[\"Year\"] == (2015) ) ) ]\n",
        "\n",
        "shared_url = \"https://drive.google.com/file/d/1z9w4rk21CCbbuOjwsru1pgkiOsYgVX6h/view?usp=share_link\"\n",
        "file_id = shared_url.split('/d/')[1].split('/')[0]\n",
        "download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "prism_df = pd.read_csv(download_url)\n",
        "prism_df['date'] = pd.to_datetime( prism_df['date'])\n",
        "\n",
        "# https://www.tensorflow.org/tutorials/structured_data/time_series\n",
        "# https://www.scirp.org/journal/paperinformation?paperid=142149\n",
        "# https://etasr.com/index.php/ETASR/article/view/10595\n",
        "\n",
        "def days_since_october_first(date):\n",
        "    october_first = pd.Timestamp(year=date.year, month=10, day=1)\n",
        "    days_difference = (date - october_first).days\n",
        "    return days_difference if days_difference >= 0 else days_difference + 365\n",
        "\n",
        "prism_df['Days_Since_October_1st'] =  prism_df['date'].apply(days_since_october_first)\n",
        "\n",
        "prism_df[\"site_no\"] =prism_df[\"site_no\"].astype(str)\n",
        "# Add a leading 0 to strings with exactly 7 characters\n",
        "prism_df['site_no'] = prism_df['site_no'].apply(lambda x: '0' + x if len(x) == 7 else x)\n",
        "prism_df[\"site_no\"] =prism_df[\"site_no\"].astype(str)\n",
        "\n",
        "prism_df['Date'] = prism_df['date'].dt.date\n",
        "\n",
        "prism_df = prism_df[['tmean' , \"site_no\", 'Date','Days_Since_October_1st'\n",
        " ]]\n",
        "\n",
        "dailyStreamtemp_df = dailyStreamtemp_df[[    'Date',       'site_no',  '00010_Mean', '00060_Mean'\n",
        "   ]]\n",
        "\n",
        "result = pd.merge(prism_df, dailyStreamtemp_df, on=['Date','site_no'], how='inner')\n",
        "\n",
        "# Count the number of NaN values in each column\n",
        "na_count = result.isna().sum()\n",
        "\n",
        "# Drop rows with any NaN values\n",
        "df_cleaned = result.dropna()\n",
        "\n",
        "df_cleaned['Date'] = pd.to_datetime(df_cleaned['Date'])\n",
        "\n",
        "train_month =  list(range( 3 , 8+1 ))\n",
        "\n",
        "print(\"train_month:\", train_month)\n",
        "\n",
        "df_cleaned= df_cleaned[ df_cleaned['Date'].dt.month.isin( train_month )]\n",
        "\n",
        "print(\"df_cleaned:\",df_cleaned)\n",
        "\n",
        "print(\" df_cleaned.describe():\",df_cleaned.describe())\n",
        "\n",
        "# Count the number of NaN values in each column\n",
        "na_count = df_cleaned.isna().sum()\n",
        "\n",
        "print(na_count)\n",
        "\n",
        "# gap\n",
        "\n",
        "# Calculate time difference between consecutive dates\n",
        "df_cleaned['Gap'] = df_cleaned['Date'].diff()\n",
        "\n",
        "# Filter rows where the gap is more than expected (e.g., > 1 day)\n",
        "expected_freq = pd.Timedelta(days=1)\n",
        "gaps = df_cleaned[df_cleaned['Gap'] > expected_freq]\n",
        "\n",
        "print(\"gaps\",gaps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d9e557a7-1608-4108-953a-59f2e4812f02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9e557a7-1608-4108-953a-59f2e4812f02",
        "outputId": "0ea04276-5800-435d-bc50-97dfe0089b03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(920, 7)\n",
            "(920, 7)\n",
            "\n",
            "DataFrame after Normalization Head:\n",
            "          tmean   site_no       Date  Days_Since_October_1st  00010_Mean  \\\n",
            "10558  0.722937  13190500 2024-08-27                0.978142    0.744186   \n",
            "10559  0.607814  13190500 2024-08-28                0.983607    0.720930   \n",
            "10560  0.641404  13190500 2024-08-29                0.989071    0.709302   \n",
            "10561  0.737673  13190500 2024-08-30                0.994536    0.732558   \n",
            "10562  0.842329  13190500 2024-08-31                1.000000    0.732558   \n",
            "\n",
            "       00060_Mean    Gap  \n",
            "10558    0.048342 1 days  \n",
            "10559    0.042626 1 days  \n",
            "10560    0.042789 1 days  \n",
            "10561    0.042300 1 days  \n",
            "10562    0.042300 1 days  \n"
          ]
        }
      ],
      "source": [
        "# --- 2. Preprocessing ---\n",
        "# Select features and target\n",
        "features = ['tmean',  '00060_Mean' ,\"Days_Since_October_1st\"]\n",
        "target = '00010_Mean'\n",
        "# Normalize the data\n",
        "# It's crucial to normalize all input features and the target variable for LSTMs.\n",
        "# We'll use a separate scaler for the target 'y' to easily inverse transform predictions.\n",
        "#  the minimum of feature is made equal to zero and the maximum of feature equal to one.\n",
        "scaler_features = MinMaxScaler(feature_range=(0, 1)) # other scalers like StandardScaler (Z-score normalization) might be more appropriate\n",
        "scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "print(df_cleaned.shape)\n",
        "df = df_cleaned.copy()\n",
        "print(df.shape)\n",
        "# Fit and transform features\n",
        "df[features] = scaler_features.fit_transform(df[features])\n",
        "\n",
        "# Fit and transform target\n",
        "# Reshape for scaler (expects 2D array)\n",
        "df[target] = scaler_target.fit_transform(df[target].values.reshape(-1, 1))\n",
        "print(\"\\nDataFrame after Normalization Head:\")\n",
        "print(df.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f1ff2931-1e64-4e59-be1f-6ac50a1dd748",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1ff2931-1e64-4e59-be1f-6ac50a1dd748",
        "outputId": "f9305c14-a154-4ecd-f03e-eb519ee2b587"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data_np: [[0.19632916 0.00620611 0.         0.06976744]\n",
            " [0.19166214 0.0052262  0.00546448 0.08139535]\n",
            " [0.33910138 0.00571615 0.01092896 0.11627907]\n",
            " ...\n",
            " [0.64140402 0.04278948 0.98907104 0.70930233]\n",
            " [0.73767264 0.04229953 0.99453552 0.73255814]\n",
            " [0.84232937 0.04229953 1.         0.73255814]]\n",
            "X [[[0.19632916 0.00620611 0.        ]\n",
            "  [0.19166214 0.0052262  0.00546448]\n",
            "  [0.33910138 0.00571615 0.01092896]\n",
            "  ...\n",
            "  [0.73914915 0.14421035 0.48087432]\n",
            "  [0.85649158 0.14584354 0.4863388 ]\n",
            "  [0.82773935 0.14421035 0.49180328]]\n",
            "\n",
            " [[0.19166214 0.0052262  0.00546448]\n",
            "  [0.33910138 0.00571615 0.01092896]\n",
            "  [0.30857123 0.00555283 0.01639344]\n",
            "  ...\n",
            "  [0.85649158 0.14584354 0.4863388 ]\n",
            "  [0.82773935 0.14421035 0.49180328]\n",
            "  [0.63928026 0.14421035 0.49726776]]\n",
            "\n",
            " [[0.33910138 0.00571615 0.01092896]\n",
            "  [0.30857123 0.00555283 0.01639344]\n",
            "  [0.33729846 0.00457292 0.02185792]\n",
            "  ...\n",
            "  [0.82773935 0.14421035 0.49180328]\n",
            "  [0.63928026 0.14421035 0.49726776]\n",
            "  [0.66326826 0.14584354 0.50273224]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[0.55652385 0.28466438 0.49726776]\n",
            "  [0.62982258 0.28303119 0.50273224]\n",
            "  [0.65785809 0.28303119 0.50819672]\n",
            "  ...\n",
            "  [0.7229368  0.04834232 0.97814208]\n",
            "  [0.60781396 0.04262616 0.98360656]\n",
            "  [0.64140402 0.04278948 0.98907104]]\n",
            "\n",
            " [[0.62982258 0.28303119 0.50273224]\n",
            "  [0.65785809 0.28303119 0.50819672]\n",
            "  [0.54671198 0.28466438 0.5136612 ]\n",
            "  ...\n",
            "  [0.60781396 0.04262616 0.98360656]\n",
            "  [0.64140402 0.04278948 0.98907104]\n",
            "  [0.73767264 0.04229953 0.99453552]]\n",
            "\n",
            " [[0.65785809 0.28303119 0.50819672]\n",
            "  [0.54671198 0.28466438 0.5136612 ]\n",
            "  [0.62578894 0.28466438 0.51912568]\n",
            "  ...\n",
            "  [0.64140402 0.04278948 0.98907104]\n",
            "  [0.73767264 0.04229953 0.99453552]\n",
            "  [0.84232937 0.04229953 1.        ]]]\n",
            "First 4 Y elements: [0.27906977 0.26744186 0.27906977 0.29069767]\n",
            "Last 4 Y elements: [0.72093023 0.70930233 0.73255814 0.73255814]\n",
            "\n",
            "Shape of X (samples, timesteps, features): (830, 91, 3)\n",
            "Shape of Y (samples, target_value): (830,)\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Create Sequences for LSTM ---\n",
        "# LSTMs require data in a 3D format: (samples, timesteps, features)\n",
        "# 'timesteps' (also known as look_back) determines how many previous time steps\n",
        "# the LSTM will consider to predict the next value.\n",
        "\n",
        "look_back =  90 # You can adjust this based on your data's seasonality/dependencies\n",
        "\n",
        "def create_sequences(data, look_back, features_cols, target_col):\n",
        "    X, Y = [], []\n",
        "    # Ensure data is a numpy array for efficient indexing\n",
        "    data_np = data[features_cols + [target_col]].values # include target for sequence alignment\n",
        "    print(\"data_np:\",data_np)\n",
        "    # Features are the first 'num_features' columns, target is the last column.\n",
        "    num_features = len(features_cols)\n",
        "\n",
        "    for i in range(len(data_np) - look_back):\n",
        "        # X: look_back historical features (x1, x2)\n",
        "        X.append(data_np[i:(i + look_back +1), :num_features]) # (i + look_back +1) will include the feature of the day of the target\n",
        "        # Y: the 'y' value at the current time step (i + look_back)\n",
        "        Y.append(data_np[i + look_back, num_features]) # Target is the last column\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "X, Y = create_sequences(df, look_back, features, target)\n",
        "\n",
        "print(\"X\", X) # did I miss today's X?\n",
        "\n",
        "# Print first 4 elements\n",
        "print(\"First 4 Y elements:\", Y[:4])\n",
        "\n",
        "# Print last 4 elements\n",
        "print(\"Last 4 Y elements:\", Y[-4:])\n",
        "\n",
        "print(f\"\\nShape of X (samples, timesteps, features): {X.shape}\")\n",
        "print(f\"Shape of Y (samples, target_value): {Y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0896b472-1e5f-4483-9414-59aa2e7eb733",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0896b472-1e5f-4483-9414-59aa2e7eb733",
        "outputId": "9a66e198-efca-4cc3-ccad-3c405096e008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train samples: 664\n",
            "Test samples: 166\n",
            " Y Train samples: 664\n",
            "Y Test samples: 166\n"
          ]
        }
      ],
      "source": [
        "# --- 4. Train-Test Split (Time Series Split) ---\n",
        "# It's crucial to split time series data chronologically to avoid data leakage.\n",
        "train_size = int(len(X) * 0.8) # e.g., 80% for training, 20% for testing\n",
        "X_train, X_test = X[0:train_size], X[train_size:len(X)]\n",
        "Y_train, Y_test = Y[0:train_size], Y[train_size:len(Y)]\n",
        "\n",
        "print(f\"Train samples: {len(X_train)}\")\n",
        "print(f\"Test samples: {len(X_test)}\")\n",
        "print(f\" Y Train samples: {len(Y_train)}\")\n",
        "print(f\"Y Test samples: {len(Y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "fa97f985-9db8-4695-b531-a8b13691e4b2",
      "metadata": {
        "id": "fa97f985-9db8-4695-b531-a8b13691e4b2"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "def create_model( h1,   d1 , h2, d2,learning_rate ):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units= h1, return_sequences=True , input_shape=(look_back, len(features))))\n",
        "    model.add(Dropout(  d1 ))  # Dropout to prevent overfitting\n",
        "    model.add(LSTM(units=h2, return_sequences=False))  # Last LSTM layer\n",
        "    model.add(Dropout(d2))\n",
        "    model.add(Dense(units=1))\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "\n",
        "    return model\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 42\n",
        "tf.random.set_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07aa5557-6dff-4f5d-8a52-345870996e5f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07aa5557-6dff-4f5d-8a52-345870996e5f",
        "outputId": "d797fd92-d605-45c9-e17d-cb63aff22fba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['model', 'build_fn', 'warm_start', 'random_state', 'optimizer', 'loss', 'metrics', 'batch_size', 'validation_batch_size', 'verbose', 'callbacks', 'validation_split', 'shuffle', 'run_eagerly', 'epochs'])\n",
            "model: KerasRegressor(\n",
            "\tmodel=<function create_model at 0x7cbfabb55120>\n",
            "\tbuild_fn=None\n",
            "\twarm_start=False\n",
            "\trandom_state=None\n",
            "\toptimizer=rmsprop\n",
            "\tloss=None\n",
            "\tmetrics=None\n",
            "\tbatch_size=None\n",
            "\tvalidation_batch_size=None\n",
            "\tverbose=0\n",
            "\tcallbacks=[<keras.src.callbacks.early_stopping.EarlyStopping object at 0x7cbfab2872d0>]\n",
            "\tvalidation_split=0.2\n",
            "\tshuffle=True\n",
            "\trun_eagerly=False\n",
            "\tepochs=34\n",
            ")\n",
            "{'model__h1': [2, 4], 'model__d1': [0.0, 0.05], 'model__h2': [2, 4], 'model__d2': [0.0, 0.05], 'model__learning_rate': [0.001, 0.01], 'batch_size': [16, 32]}\n",
            "GridSearchCV(estimator=KerasRegressor(callbacks=[<keras.src.callbacks.early_stopping.EarlyStopping object at 0x7cbfab2872d0>], epochs=34, model=<function create_model at 0x7cbfabb55120>, validation_split=0.2, verbose=0),\n",
            "             n_jobs=-1,\n",
            "             param_grid={'batch_size': [16, 32], 'model__d1': [0.0, 0.05],\n",
            "                         'model__d2': [0.0, 0.05], 'model__h1': [2, 4],\n",
            "                         'model__h2': [2, 4],\n",
            "                         'model__learning_rate': [0.001, 0.01]})\n"
          ]
        }
      ],
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True) #Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n",
        "max_epochs =34\n",
        "\n",
        "# create model\n",
        "model = KerasRegressor(model=create_model\n",
        "                       , epochs=  max_epochs\n",
        "                       , callbacks=[early_stop]\n",
        "                      , validation_split=0.2\n",
        "                       #, batch_size=10\n",
        "                       , verbose=0)\n",
        "print(model.get_params().keys())\n",
        "print(\"model:\",model)\n",
        "# define the grid search parameters\n",
        "h1 = [     2,4\n",
        "      #,8,16,32,64,128,256\n",
        "           ]\n",
        "h2 = [     2,4\n",
        "#      ,8,16,32,64,128,256\n",
        "           ]\n",
        "d1 = [0.0,0.05\n",
        "      #, 0.1,0.2\n",
        "      ]\n",
        "d2 = [ 0.0,0.05\n",
        "      #, 0.1,0.2\n",
        "       ]\n",
        "learning_rate=[\n",
        "    #0.0001,\n",
        "    0.001,  0.01]\n",
        "batch_size = [\n",
        "    #4,8,\n",
        "              16,32\n",
        "               ]\n",
        "\n",
        "param_grid = dict( model__h1= h1 ,model__d1= d1, model__h2=h2 , model__d2=d2 , model__learning_rate= learning_rate, batch_size=batch_size )\n",
        "print(param_grid)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1\n",
        "                    #, cv=3\n",
        "                    )\n",
        "print(grid)\n",
        "grid_result = grid.fit(X_train, Y_train)\n",
        "print(\"grid_result:\",grid_result)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "#means = grid_result.cv_results_['mean_test_score']\n",
        "#stds = grid_result.cv_results_['std_test_score']\n",
        "#params = grid_result.cv_results_['params']\n",
        "#for mean, stdev, param in zip(means, stds, params):\n",
        "#    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Make Predictions ---\n",
        "# Make predictions on the test set\n",
        "best_params= grid_result.best_params_\n",
        "\n",
        "print(\"best_params:\",best_params)\n",
        "\n",
        "#print(\"best_params:\",best_params.keys() )\n",
        "\n",
        "#print(\"best_params:\",best_params['model__d1'] )\n",
        "\n",
        "# Enable eager execution\n",
        "#tf.config.experimental_run_eagerly(True)\n",
        "\n",
        "best_model = create_model(  best_params['model__h1'], best_params['model__d1'], best_params['model__h2'], best_params['model__d2'] , best_params['model__learning_rate']   )\n",
        "\n",
        "print(best_model.summary())\n",
        "\n",
        "#early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True\n",
        "\n",
        "print(early_stop)\n",
        "\n",
        "#best_model = create_model(**{k: best_params[k] for k in ['model__h1', 'model__d1', 'model__h2', 'model__d2' ]})\n",
        "best_model.fit(X_train, Y_train, epochs=100 # best_params['epochs']\n",
        "               , callbacks=[early_stop]\n",
        "               , batch_size=best_params['batch_size']\n",
        "               )\n",
        "\n",
        "Y_pred_scaled = best_model.predict(X_test)\n",
        "print(f\"\\nShape of Y_pred_scaled: {Y_pred_scaled.shape}\")\n",
        "print(f\"Shape of Y_test: {Y_test.shape}\")\n",
        "print(f\"Shape of X_test: {X_test.shape}\")\n",
        "\n",
        "# Inverse transform the predictions and actual values to the original scale\n",
        "Y_pred = scaler_target.inverse_transform(Y_pred_scaled)\n",
        "Y_test_original = scaler_target.inverse_transform(Y_test.reshape(-1, 1))\n",
        "\n",
        "r2 = r2_score( Y_test_original ,  Y_pred )\n",
        "\n",
        "# Calculate RMSE\n",
        "all_rmse = np.sqrt(mean_squared_error(Y_test_original, Y_pred))\n",
        "print(f\"Test RMSE: {all_rmse:.4f}\")\n",
        "\n",
        "bias =np.mean( Y_pred- Y_test_original)\n",
        "print(f\"Test bias: { bias:.4f}\")\n",
        "\n",
        "# KGE calculation using hydroeval\n",
        "# Note: hydroeval's kge function returns KGE, r, alpha, beta by default\n",
        "# kge_value, r_comp, alpha_comp, beta_comp = he.kge( Y_pred , Y_test_original)\n",
        "#print( kge_value)\n",
        "\n",
        "std_ratio = np.std( Y_pred ) / np.std( Y_test_original ) # standard deviation ratio\n",
        "correlation = np.corrcoef( Y_pred.flatten() ,  Y_test_original.flatten() )[0, 1]\n",
        "print(\"correlation\", correlation)\n",
        "\n",
        "print(np.std(Y_pred))\n",
        "print(np.std(Y_test_original))"
      ],
      "metadata": {
        "id": "BivEaA6cvWg-"
      },
      "id": "BivEaA6cvWg-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_cleaned.tail:\",df_cleaned.tail)\n",
        "print((len(Y)-Y_test.shape[0]  ))\n",
        "print(len(Y) )\n",
        "dt_ys = df_cleaned.iloc[  ( df_cleaned.shape[0]-Y_test.shape[0]  )  : df_cleaned.shape[0] ]#['Date']\n",
        "\n",
        "dt_ys['Y_pred'] = Y_pred\n",
        "dt_ys['Y_test_original']= Y_test_original\n",
        "\n",
        "dt_ys[['Date','Y_pred', \t'Y_test_original']]\n",
        "\n",
        "dt_ys['Date'] = pd.to_datetime( dt_ys['Date'])\n",
        "\n",
        "print( \"dt_ys.tail()\", dt_ys.tail() )\n",
        "\n",
        "print( dt_ys['Date'].dt.month.describe())\n",
        "\n",
        "summer_df = dt_ys[ dt_ys['Date'].dt.month.isin( summer_months)]\n",
        "\n",
        "print(\"summer_df.tail()\",summer_df.tail())\n",
        "\n",
        "summer_df.to_csv( '/content/drive/My Drive/LSTM_DOY/LSTM_predictions_DOY_grid_search/'+( model_site[0] + \"summer_df.csv\") , index=False )\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(summer_df['Y_test_original'], summer_df['Y_pred']))\n",
        "print(f\"Test RMSE: {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "Qv_X3enUjhMR"
      },
      "id": "Qv_X3enUjhMR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec64ad7-fe01-429f-a9ab-1f1e588ac11d",
      "metadata": {
        "id": "eec64ad7-fe01-429f-a9ab-1f1e588ac11d"
      },
      "outputs": [],
      "source": [
        "from numpy import array\n",
        "!pip install permetrics\n",
        "from permetrics.regression import RegressionMetric\n",
        "\n",
        "## For 1-D array\n",
        "y_true = array([3, -0.5, 2, 7])\n",
        "y_pred = array([2.5, 0.0, 2, 8])\n",
        "\n",
        "evaluator = RegressionMetric(y_true, y_pred)\n",
        "print(evaluator.nash_sutcliffe_efficiency())\n",
        "\n",
        "## For > 1-D array\n",
        "y_true = array([[0.5, 1], [-1, 1], [7, -6]])\n",
        "y_pred = array([[0, 2], [-1, 2], [8, -5]])\n",
        "\n",
        "evaluator = RegressionMetric(y_true, y_pred)\n",
        "print(evaluator.NSE(multi_output=\"raw_values\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}